{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "from synthcity.metrics.eval_performance import (\n",
    "    PerformanceEvaluatorMLP,\n",
    "    PerformanceEvaluatorXGB,\n",
    ")\n",
    "from synthcity.utils import reproducibility\n",
    "from synthcity.plugins import Plugins\n",
    "import synthcity.logger as log\n",
    "from synthcity.plugins.core.dataloader import GenericDataLoader\n",
    "from deep_generative_ensemble.DGE_utils import metric_different_datasets, mean_across_pandas, add_std, get_folder_names\n",
    "\n",
    "reproducibility.clear_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "assert device.type == 'cuda'\n",
    "Plugins(categories=[\"generic\"]).list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_generative_ensemble.DGE_data import get_real_and_synthetic\n",
    "\n",
    "# let's restrict ourselves to classification datasets\n",
    "datasets = ['moons', 'circles', 'breast_cancer',\n",
    "            'adult',  'seer', 'covid']\n",
    "model_name = 'tvae'  # synthetic data model\n",
    "\n",
    "n_models = 20  # number of models in ensemble, for each run\n",
    "max_n = 2000  # maximum number of data points to use for training generative model.\n",
    "nsyn = 2000  # number of synthetic data points per synthetic dataset. Defaults to same as generative training size if None\n",
    "\n",
    "num_runs = 10 # Number of runs. Don't choose to large, since total number of synthetic datasets is num_runs*n_models\n",
    "\n",
    "# Whether to load and save models and synthetic datasets\n",
    "load = True  # results\n",
    "load_syn = True  # data\n",
    "save = True  # save results and data\n",
    "\n",
    "verbose = False\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_generative_ensemble.DGE_experiments import predictive_experiment\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "all_means = {}\n",
    "all_stds = {}\n",
    "\n",
    "datasets = ['moons', 'circles', 'breast_cancer', 'adult', 'covid', 'seer']\n",
    "num_runs=10\n",
    "model_type = 'deepish_mlp'\n",
    "\n",
    "for dataset in datasets:\n",
    "    workspace_folder, results_folder = get_folder_names(\n",
    "        dataset, model_name, max_n=max_n, nsyn=nsyn)\n",
    "\n",
    "    X_gt, X_syns = get_real_and_synthetic(dataset=dataset,\n",
    "                                          n_models=n_models*num_runs,\n",
    "                                          model_name=model_name,\n",
    "                                          load_syn=load_syn,\n",
    "                                          verbose=verbose,\n",
    "                                          max_n=max_n,\n",
    "                                          nsyn=nsyn)\n",
    "\n",
    "\n",
    "    print(f'Dataset {dataset}\\n')\n",
    "\n",
    "    means, stds, _ = predictive_experiment(X_gt,\n",
    "                                            X_syns,\n",
    "                                            workspace_folder=workspace_folder,\n",
    "                                            results_folder=results_folder,\n",
    "                                            save=save,\n",
    "                                            load=load,\n",
    "                                            plot=False,\n",
    "                                            outlier=False, \n",
    "                                            task_type = model_type\n",
    "                                            )\n",
    "\n",
    "    print(means.to_latex())\n",
    "\n",
    "    all_means[dataset] = means\n",
    "    all_stds[dataset] = stds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results, aggregated over different datasets\n",
    "means_consolidated = metric_different_datasets(all_means, to_print=False)\n",
    "if num_runs>1:\n",
    "    stds_consolidated = metric_different_datasets(all_stds, to_print=False)\n",
    "    stds_consolidated.drop(columns=['Mean'], inplace=True)\n",
    "    print(add_std(means_consolidated, stds_consolidated).to_latex())\n",
    "else:\n",
    "    print(means_consolidated.to_latex())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run over generative model sizes\n",
    "Let us study what happens when we run the generative model for different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_generative_ensemble.DGE_experiments import predictive_experiment\n",
    "import pandas as pd\n",
    "genrun_all = {}\n",
    "dataset = 'seer'\n",
    "\n",
    "max_n = 5000\n",
    "nsyn = 5000\n",
    "model_type = 'deepish_mlp'\n",
    "num_runs = 10\n",
    "\n",
    "for layers, model_name in zip(range(1,4),['ctgan_shallow', 'ctgan', 'ctgan_deep']):\n",
    "\n",
    "    workspace_folder, results_folder = get_folder_names(\n",
    "            dataset, model_name, max_n=max_n, nsyn=nsyn)\n",
    "\n",
    "    X_gt, X_syns = get_real_and_synthetic(dataset=dataset,\n",
    "                                        n_models=n_models*num_runs,\n",
    "                                        model_name=model_name,\n",
    "                                        load_syn=load_syn,\n",
    "                                        verbose=verbose,\n",
    "                                        max_n=max_n,\n",
    "                                        nsyn=nsyn)\n",
    "\n",
    "    print(f'Model {model_name}\\n')\n",
    "\n",
    "    means, stds, all = predictive_experiment(X_gt,\n",
    "                                            X_syns,\n",
    "                                            workspace_folder=workspace_folder,\n",
    "                                            results_folder=results_folder,\n",
    "                                            task_type=model_type,\n",
    "                                            save=save,\n",
    "                                            load=load,\n",
    "                                            plot=False,\n",
    "                                            )\n",
    "\n",
    "    all['Hidden layers'] = layers\n",
    "    genrun_all[layers] = all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dfs = pd.concat(genrun_all)\n",
    "oracle = dfs[dfs['Approach']=='Oracle']\n",
    "dfs = dfs[(dfs['Approach']==\"DGE$_{20}$\")|(dfs['Approach']=='Naive (E)')]\n",
    "\n",
    "metric = 'Acc'\n",
    "\n",
    "sns.catplot(x='Hidden layers', y=metric, data=dfs, kind='box', hue='Approach', aspect = 2, height=3, showfliers = False, legend=False)\n",
    "plt.tight_layout()\n",
    "#draw line for oracle mean\n",
    "plt.axhline(oracle[metric].mean(), color='black', linestyle='--', label='$\\mathcal{D}_R$-model')\n",
    "plt.legend()\n",
    "os.makedirs('results/stories', exist_ok=True)\n",
    "plt.savefig(f'results/stories/training_models_{dataset}_max_n{max_n}_nsyn_{nsyn}_{model_type}_{metric}.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "We compare the single baseline model vs the generative uncertainty model vs an oracle. Workflow.\n",
    "0. Train and generate synthetic datasets $S_i$.\n",
    "1. Take each synthetic dataset $S_i$ and split it up in train and test.\n",
    "2. Train a model $f_i$ on the train set, for each $S_i$\n",
    "3. Evaluate on the same synthetic dataset's test set $S_{i,test}$, giving $\\hat{M}^S_i$ [Naive evaluation]\n",
    "4. Evaluate on the true real test set (oracle), $D_{test}$, giving $M_i$ [Oracle evaluation]\n",
    "5. Evaluate on the other synthetic datasets $\\cup_{j\\neq i} S_{j}$, giving $\\hat{M}^G_i$ [DGE evaluation]\n",
    "6. Average over all models $f_i$. \n",
    "\n",
    "N.B. the idea of the above, is that the trained model $f_i$ is the same for each evaluation type. In the model selection section, we will compare the performance of different model classes, where we will train a new model for each evaluation type (hence the aim is to evaluate which class is best, while the model itself may vary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_generative_ensemble.DGE_experiments import model_evaluation_experiment\n",
    "\n",
    "evaluation_means = {}\n",
    "evaluation_std = {}\n",
    "relative = False\n",
    "max_n = 5000\n",
    "nsyn = 5000\n",
    "model_name = 'ctgan_deep'\n",
    "datasets = ['moons', 'circles', 'adult', 'seer', 'covid']\n",
    "model_type = 'deepish_mlp'\n",
    "verbose = False\n",
    "\n",
    "for dataset in datasets:\n",
    "    print('Dataset:', dataset)\n",
    "    workspace_folder, results_folder = get_folder_names(\n",
    "        dataset, model_name, max_n=max_n, nsyn=nsyn)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # load data\n",
    "    X_gt, X_syns = get_real_and_synthetic(dataset=dataset,\n",
    "                                          n_models=n_models,\n",
    "                                          model_name=model_name,\n",
    "                                          load_syn=load_syn,\n",
    "                                          verbose=verbose,\n",
    "                                          max_n=max_n,\n",
    "                                          nsyn=nsyn)\n",
    "\n",
    "    # get mean and std of dataset over different runs\n",
    "    means, std, _ = model_evaluation_experiment(X_gt, X_syns, workspace_folder=workspace_folder, relative=relative,\n",
    "                                             model_type=model_type,\n",
    "                                             load=load,\n",
    "                                             save=load,\n",
    "                                             verbose=verbose,\n",
    "                                             )\n",
    "\n",
    "    evaluation_means[dataset] = means\n",
    "    evaluation_std[dataset] = std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean across datasets\n",
    "# per dataset\n",
    "metric = 'Acc'\n",
    "res = metric_different_datasets(evaluation_means, metric=metric, to_print=False)\n",
    "std_df = metric_different_datasets(evaluation_std, metric=metric, to_print=False)\n",
    "\n",
    "del std_df['Mean']\n",
    "res = add_std(res, std_df)\n",
    "if relative != 'l2':\n",
    "    print(res.to_latex(float_format=lambda x: '%.3f' % x))\n",
    "else:\n",
    "    print(res.to_latex(float_format=lambda x: '%.5f' % x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vary the depth of the generative model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_generative_ensemble.DGE_experiments import model_evaluation_experiment\n",
    "\n",
    "genrun_means = {}\n",
    "genrun_stds = {}\n",
    "genrun_all = {}\n",
    "dataset = 'seer'\n",
    "relative = False\n",
    "verbose = False\n",
    "\n",
    "max_n = 5000\n",
    "nsyn = 5000\n",
    "n_models = 20\n",
    "\n",
    "for layers, model_name in zip(range(1,4),['ctgan_shallow', 'ctgan', 'ctgan_deep']):\n",
    "\n",
    "    workspace_folder, results_folder = get_folder_names(\n",
    "            dataset, model_name, max_n=max_n, nsyn=nsyn)\n",
    "\n",
    "    X_gt, X_syns = get_real_and_synthetic(dataset=dataset,\n",
    "                                        p_train=p_train,\n",
    "                                        n_models=n_models,\n",
    "                                        model_name=model_name,\n",
    "                                        load_syn=load_syn,\n",
    "                                        verbose=verbose,\n",
    "                                        max_n=max_n,\n",
    "                                        nsyn=nsyn)\n",
    "\n",
    "\n",
    "    _, _, all = model_evaluation_experiment(X_gt, X_syns, workspace_folder=workspace_folder, relative=relative,\n",
    "                                             model_type='deep_mlp',\n",
    "                                             load=load,\n",
    "                                             save=load,\n",
    "                                             verbose=verbose,\n",
    "                                             outlier=outlier,\n",
    "                                             )\n",
    "\n",
    "    all['Hidden layers'] = layers\n",
    "    genrun_all[layers] = all\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metric = 'AUC'\n",
    "dfs = pd.concat(genrun_all,axis=0)\n",
    "dfs.reset_index(inplace=True)\n",
    "dfs['Hidden layers'] = dfs['level_0']\n",
    "dfs = dfs[(dfs['Approach']==\"DGE$_{20}$\")|(dfs['Approach']==\"DGE (K=20)\")|(dfs['Approach']=='Naive')|(dfs['Approach']=='Oracle')]\n",
    "sns.catplot(x='Hidden layers', y=metric, data=dfs, kind='box', hue='Approach', aspect=2, height = 3, showfliers=False, legend=False)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'results/stories/eval_models_{dataset}_{metric}.png', dpi=300)\n",
    "#draw line for oracle mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_generative_ensemble.DGE_experiments import model_evaluation_experiment\n",
    "\n",
    "nsynrun_all = {}\n",
    "dataset = 'seer'\n",
    "relative = False\n",
    "\n",
    "max_n = 5000\n",
    "model_name = 'ctgan_deep'\n",
    "n_models = 20\n",
    "verbose = False\n",
    "for nsyn in [1000, 5000, 20000]:\n",
    "\n",
    "    workspace_folder, results_folder = get_folder_names(\n",
    "            dataset, model_name, max_n=max_n, nsyn=nsyn)\n",
    "\n",
    "    X_gt, X_syns = get_real_and_synthetic(dataset=dataset,\n",
    "                                        p_train=p_train,\n",
    "                                        n_models=n_models,\n",
    "                                        model_name=model_name,\n",
    "                                        load_syn=load_syn,\n",
    "                                        verbose=verbose,\n",
    "                                        max_n=max_n,\n",
    "                                        nsyn=nsyn)\n",
    "\n",
    "   \n",
    "    _, _, all = model_evaluation_experiment(X_gt, X_syns, workspace_folder=workspace_folder, relative=relative,\n",
    "                                             model_type='deep_mlp',\n",
    "                                             load=load,\n",
    "                                             save=load,\n",
    "                                             verbose=verbose,\n",
    "                                             outlier=outlier,\n",
    "                                             )\n",
    "\n",
    "    all['nsyn'] = nsyn\n",
    "    nsynrun_all[nsyn] = all\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dfs = pd.concat(nsynrun_all, axis=0)\n",
    "\n",
    "metric = 'AUC'\n",
    "dfs.reset_index(inplace=True)\n",
    "dfs['Synthetic dataset size'] = dfs['level_0']\n",
    "relative = False\n",
    "\n",
    "if relative:\n",
    "    oracle = dfs[(dfs['Approach']=='Oracle')]\n",
    "    dfs = dfs[(dfs['Approach']==\"DGE$_{20}$\")|(dfs['Approach']==\"DGE (K=20)\")|(dfs['Approach']=='Naive')]\n",
    "    dfs -= oracle\n",
    "else:\n",
    "    dfs = dfs[(dfs['Approach']==\"DGE$_{20}$\")|(dfs['Approach']==\"DGE (K=20)\")|(dfs['Approach']=='Naive')|(dfs['Approach']=='Oracle')]\n",
    "sns.catplot(x='Synthetic dataset size', y=metric, data=dfs, kind='box', hue='Approach', aspect=2, height = 3, showfliers=False, legend=False)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig(f'results/stories/eval_nsyn_{dataset}_{metric}.png', dpi=300)\n",
    "#draw line for oracle mean\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "Essentially repeat the above for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_generative_ensemble.DGE_experiments import model_selection_experiment\n",
    "\n",
    "# load data\n",
    "dataset = 'seer'\n",
    "max_n = 5000\n",
    "nsyn = 5000\n",
    "X_gt, X_syns = get_real_and_synthetic(dataset=dataset,\n",
    "                                        n_models=n_models,\n",
    "                                        model_name=model_name,\n",
    "                                        load_syn=load_syn,\n",
    "                                        verbose=verbose,\n",
    "                                        max_n=max_n,\n",
    "                                        nsyn=nsyn)\n",
    "\n",
    "\n",
    "\n",
    "workspace_folder, results_folder = get_folder_names(\n",
    "    dataset, model_name, max_n=max_n, nsyn=nsyn)\n",
    "\n",
    "means_sorted, std = model_selection_experiment(X_gt, X_syns, relative=False,\n",
    "                                                    workspace_folder=workspace_folder, \n",
    "                                                    load=load, save=save)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = add_std(means_sorted[metric].iloc[:5], std[metric])\n",
    "B = means_sorted[metric].iloc[5:]\n",
    "B.index = [f'{i}'.replace(' rank','') for i in B.index]\n",
    "B = B.astype(int)\n",
    "B.columns = ['' for i in B.columns]\n",
    "C = pd.concat([A, B], axis=1)\n",
    "print(C.to_latex())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run for different synthetic dataset size (Appendix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_generative_ensemble.DGE_experiments import model_selection_experiment\n",
    "from deep_generative_ensemble.DGE_data import get_real_and_synthetic\n",
    "from deep_generative_ensemble.DGE_utils import get_folder_names\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "# load data\n",
    "dataset = 'seer'\n",
    "max_n = 5000\n",
    "p_train = 0.8\n",
    "n_models = 20\n",
    "model_name = 'ctgan_deep'\n",
    "load=True\n",
    "save = True\n",
    "load_syn = True\n",
    "verbose = False\n",
    "means = {}\n",
    "stds = {}\n",
    "model_types = ['smallest_mlp','mlp', 'deepish_mlp', 'deep_mlp', 'largest_mlp']\n",
    "\n",
    "for nsyn in [1000, 2000, 5000,10000,20000]:\n",
    "    X_gt, X_syns = get_real_and_synthetic(dataset=dataset,\n",
    "                                            p_train=p_train,\n",
    "                                            n_models=n_models,\n",
    "                                            model_name=model_name,\n",
    "                                            load_syn=load_syn,\n",
    "                                            verbose=verbose,\n",
    "                                            max_n=max_n,\n",
    "                                            nsyn=nsyn)\n",
    "\n",
    "\n",
    "\n",
    "    workspace_folder, results_folder = get_folder_names(\n",
    "        dataset, model_name, max_n=max_n, nsyn=nsyn)\n",
    "\n",
    "    means_sorted, std = model_selection_experiment(X_gt, X_syns, relative=False,\n",
    "                                                        workspace_folder=workspace_folder, \n",
    "                                                        load=load, save=save, model_types=model_types )\n",
    "\n",
    "    means[nsyn] = means_sorted\n",
    "    stds[nsyn] = std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = means.keys()\n",
    "metric = 'AUC'\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt\n",
    "plt=reload(plt)\n",
    "for i in range(len(means_sorted[metric])):    \n",
    "    y = [means[j][metric].iloc[i] for j in x]\n",
    "    labels = means[5000][metric].columns\n",
    "    plt.semilogx(x, y, label=labels)\n",
    "    plt.xlabel('Synthetic dataset size')\n",
    "    name = means[5000][metric].index[i]\n",
    "    if 'rank' in name:\n",
    "        plt.ylabel('Ranking')\n",
    "    else:\n",
    "        plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.savefig(f'results/stories/model_selection_mlps_nsyn_{name}_{metric}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underrepresented group"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_generative_ensemble.DGE_data import get_real_and_synthetic\n",
    "dataset = 'covid'\n",
    "n_models = 20\n",
    "num_runs = 10\n",
    "max_n = 2000\n",
    "nsyn = 2000\n",
    "model_name = 'ctgan'\n",
    "load = True\n",
    "save = True\n",
    "load_syn = True\n",
    "verbose = False\n",
    "reduce_to = None # used to get a very large set of real samples for evaluation (to accurately performance on small subgroups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gt, X_syns = get_real_and_synthetic(dataset=dataset,\n",
    "                                        n_models=n_models*num_runs,\n",
    "                                        model_name=model_name,\n",
    "                                        load_syn=load_syn,\n",
    "                                        verbose=verbose,\n",
    "                                        max_n=max_n,\n",
    "                                        nsyn=nsyn,\n",
    "                                        reduce_to=reduce_to)\n",
    "\n",
    "# Get some indication of the distribution of the data\n",
    "X_gt.train().data.hist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the minority category for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from synthcity.plugins.core.dataloader import GenericDataLoader\n",
    "\n",
    "def find_minority(data, feature):\n",
    "    # Get the value that is the minority\n",
    "\n",
    "    # turn data into dataframe if GenericDataLoader\n",
    "    if type(data) != pd.DataFrame:\n",
    "        data = data.data\n",
    "    # find minority value\n",
    "    if data[feature].unique().shape[0] < 20:\n",
    "\n",
    "        counts = data[feature].value_counts()\n",
    "        percentages = counts/counts.sum()\n",
    "        counts = counts[percentages>0.005]\n",
    "        value = counts.index[-1]\n",
    "        percentage = percentages[value]\n",
    "        # Create function that selects the minority value\n",
    "        def subset(X):\n",
    "            Xout = X[X[feature]==value]\n",
    "            if type(Xout) == pd.DataFrame:\n",
    "                Xout = GenericDataLoader(Xout, target='target')\n",
    "                if hasattr(X, 'targettype'):\n",
    "                    Xout.targettype = X.targettype\n",
    "            return Xout\n",
    "    else:\n",
    "        quantile = 0.1\n",
    "        threshold = data[feature].quantile(1-quantile)\n",
    "        value = f'>={threshold:.2f}'\n",
    "        percentage = quantile\n",
    "        # Create function that selects the minority value\n",
    "        def subset(X):\n",
    "            Xout = X[X[feature]>=threshold]\n",
    "            if type(Xout) == pd.DataFrame:\n",
    "                Xout = GenericDataLoader(Xout, target='target')\n",
    "                if hasattr(X, 'targettype'):\n",
    "                    Xout.targettype = X.targettype\n",
    "            return Xout\n",
    "\n",
    "    return subset, value, percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_generative_ensemble.DGE_experiments import predictive_experiment\n",
    "from deep_generative_ensemble.DGE_utils import get_folder_names\n",
    "from synthcity.plugins.core.dataloader import GenericDataLoader\n",
    "\n",
    "values = {}\n",
    "results = {}\n",
    "percentages = {}\n",
    "\n",
    "workspace_folder, results_folder = get_folder_names(\n",
    "        dataset, model_name, max_n=max_n, nsyn=nsyn)\n",
    "\n",
    "feature = 'ALL'\n",
    "features = X_gt.unpack(as_numpy=False)[0].columns\n",
    "\n",
    "_, _, res = predictive_experiment(X_gt,\n",
    "                                    X_syns,\n",
    "                                    workspace_folder=workspace_folder,\n",
    "                                    results_folder=results_folder,\n",
    "                                    save=save,\n",
    "                                    load=load,\n",
    "                                    plot=False,\n",
    "                                    outlier=False\n",
    "                                    )\n",
    "\n",
    "results[feature] = res\n",
    "\n",
    "\n",
    "for feature in features:\n",
    "    subset, value, percentage = find_minority(X_gt.train(), feature)\n",
    "    print(f'Feature {feature},\\n minority value {value},\\n minority percentage {percentage}')\n",
    "    if percentage > 0.2:\n",
    "        print('skipping due to not being minority')\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        _, _, res = predictive_experiment(X_gt,\n",
    "                                            X_syns,\n",
    "                                            workspace_folder=workspace_folder,\n",
    "                                            results_folder=results_folder,\n",
    "                                            save=save,\n",
    "                                            load=load,\n",
    "                                            plot=False,\n",
    "                                            outlier=subset\n",
    "                                            )\n",
    "    except ValueError:\n",
    "        print('Skipping due to too heavy imbalance making AUC non-computable')\n",
    "        continue\n",
    "\n",
    "    percentages[feature] = percentage\n",
    "    values[feature] = value\n",
    "    results[feature] = res\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add the category name to each subset (just for plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_copy = results.copy() # to got back if necessary\n",
    "\n",
    "sorted_percentages = sorted(percentages.items(), key=lambda item: item[1])\n",
    "sorted_percentages = dict(sorted_percentages)\n",
    "#results_ = {f'{key}:{value} ({round(100*sorted_percentages[key],1)}%)': results[key] for key in list(sorted_percentages.keys())[::-1]}\n",
    "results_ = {f'{key}:{values[key]}': results[key] for key in list(sorted_percentages.keys())[::-1]}\n",
    "results_ = dict({'Overall':results['ALL']}, **results_)\n",
    "results = results_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "normalize = True\n",
    "if normalize:\n",
    "    results_ = {}\n",
    "    for key, res in results.items():\n",
    "        results_key = []\n",
    "        for group in res.groupby('Approach'):\n",
    "            res_ = group[1].drop(columns='Approach')-res[res['Approach']=='Oracle'].drop(columns='Approach')\n",
    "            res_['Approach'] = group[0]\n",
    "            results_key.append(res_)\n",
    "        results_key = pd.concat(results_key, axis=0)\n",
    "        results_[key] = results_key\n",
    "else:\n",
    "    results_ = results.copy()\n",
    "\n",
    "metric = 'AUC'\n",
    "baseline = 'E'\n",
    "\n",
    "df = pd.concat(results_)\n",
    "df['Subset'] = df.index.get_level_values(0)\n",
    "\n",
    "if normalize:\n",
    "    df = df[(df['Approach']=='DGE$_{20}$')|(df['Approach']==f'Naive ({baseline})')] #|(df['Approach']=='DGE$_{20}$ (concat)')\n",
    "else:\n",
    "    df = df[(df['Approach']=='DGE$_{20}$')|(df['Approach']==f'Naive ({baseline})')|(df['Approach']=='Oracle')] #|(df['Approach']=='DGE$_{20}$ (concat)')\n",
    "# draw a horizontal line at y=0\n",
    "g = sns.catplot(data=df, hue='Approach', y=metric, x='Subset', kind='box', aspect=3, showfliers=False, legend=False)\n",
    "g.set_xticklabels(rotation=20)\n",
    "if normalize:\n",
    "    plt.axhline(y=0, color='k', linestyle='--')\n",
    "    g.set_ylabels(metric+r' on subset relative to $\\mathcal{D}_R$-model')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig(f'results/stories/underrepresented_training_nsyn{nsyn}_nmax{max_n}_{metric}_{baseline}.png', dpi=300)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_generative_ensemble.DGE_data import get_real_and_synthetic\n",
    "dataset = 'covid'\n",
    "n_models = 20\n",
    "max_n = 5000\n",
    "nsyn = 5000\n",
    "model_name = 'ctgan_deep'\n",
    "model_type = 'deepish_mlp'\n",
    "load = True\n",
    "save = True\n",
    "load_syn = True\n",
    "verbose = False\n",
    "reduce_to = None\n",
    "p_train = 0.8\n",
    "\n",
    "X_gt, X_syns = get_real_and_synthetic(dataset=dataset,\n",
    "                                        p_train=p_train,\n",
    "                                        n_models=n_models,\n",
    "                                        model_name=model_name,\n",
    "                                        load_syn=load_syn,\n",
    "                                        verbose=verbose,\n",
    "                                        max_n=max_n,\n",
    "                                        nsyn=nsyn,\n",
    "                                        reduce_to=reduce_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_generative_ensemble.DGE_experiments import model_evaluation_experiment\n",
    "from deep_generative_ensemble.DGE_utils import get_folder_names\n",
    "from synthcity.plugins.core.dataloader import GenericDataLoader\n",
    "\n",
    "results = {}\n",
    "workspace_folder, results_folder = get_folder_names(\n",
    "        dataset, model_name, max_n=max_n, nsyn=nsyn)\n",
    "\n",
    "feature = 'ALL'\n",
    "features = X_gt.data.columns\n",
    "\n",
    "means, stds, res = model_evaluation_experiment(X_gt, X_syns, workspace_folder=workspace_folder, relative=False,\n",
    "                                             model_type=model_type,\n",
    "                                             load=load,\n",
    "                                             save=load,\n",
    "                                             verbose=verbose,\n",
    "                                             outlier=False,\n",
    "                                             )\n",
    "\n",
    "results[feature] = res\n",
    "\n",
    "\n",
    "for feature in features:\n",
    "    subset, value, percentage = find_minority(X_gt.train(), feature)\n",
    "    print(f'Feature {feature},\\n minority value {value},\\n minority percentage {percentage}')\n",
    "    if percentage > 0.2:\n",
    "        print('skipping due to not being minority')\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        means, stds, res = model_evaluation_experiment(X_gt, X_syns, workspace_folder=workspace_folder, relative=False,\n",
    "                                             model_type=model_type,\n",
    "                                             load=load,\n",
    "                                             save=load,\n",
    "                                             verbose=verbose,\n",
    "                                             outlier=subset,\n",
    "                                             )\n",
    "    except ValueError:\n",
    "        print('Skipping due to too heavy imbalance making AUC uncomputable')\n",
    "        continue\n",
    "\n",
    "    results[f'{feature}:{value}'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "normalize = True\n",
    "#results.pop('distance')\n",
    "if normalize:\n",
    "    results_ = {}\n",
    "    for key, res in results.items():\n",
    "        results_key = []\n",
    "        #res = res.reset_index().drop(columns=['level_0', 'level_1'])\n",
    "        res.index = [0] * len(res)\n",
    "        for group in res.groupby('Approach'):\n",
    "            res_ = group[1].drop(columns='Approach')-res[res['Approach']=='Oracle'].drop(columns='Approach')\n",
    "            res_['Approach'] = group[0]\n",
    "            results_key.append(res_)\n",
    "        results_key = pd.concat(results_key, axis=0)\n",
    "        results_[key] = results_key\n",
    "else:\n",
    "    results_ = results\n",
    "\n",
    "metric = 'Acc'\n",
    "df = pd.concat(results_)\n",
    "df['feature'] = df.index.get_level_values(0)\n",
    "if normalize:\n",
    "    df = df[(df['Approach']=='DGE$_{20}$')|(df['Approach']=='Naive')] #|(df['Approach']=='DGE$_{20}$ (concat)')\n",
    "else:\n",
    "    df = df[(df['Approach']=='DGE$_{20}$')|(df['Approach']=='Naive')|(df['Approach']=='Oracle')] #|(df['Approach']=='DGE$_{20}$ (concat)')\n",
    "    \n",
    "g = sns.catplot(data=df, hue='Approach', y=metric, x='feature', kind='box', aspect=2)\n",
    "g.set_xticklabels(rotation=30)\n",
    "# draw a horizontal line at y=0\n",
    "if normalize:\n",
    "    plt.axhline(y=0, color='k', linestyle='--')\n",
    "plt.savefig(f'results/stories/underrepresented_eval_{metric}_nsyn{nsyn}_nmax{max_n}_{model_name}_{model_type}.png')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of synthetic data size (uncertainty plots and confidence curves)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's study the effect of synthetic data size. Resulting plots are saved in /results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_generative_ensemble.DGE_experiments import predictive_experiment\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "all_means = {}\n",
    "all_stds = {}\n",
    "model_name = 'ctgan'\n",
    "n_models = 20\n",
    "max_n = 2000\n",
    "model_type = 'mlp'\n",
    "\n",
    "for dataset in ['adult', 'seer', 'covid', 'gaussian', 'circles', 'moons']:\n",
    "    for nsyn in [2000, 5000, 10000, 20000]:\n",
    "        workspace_folder, results_folder = get_folder_names(\n",
    "            dataset, model_name, max_n=max_n, nsyn=nsyn)\n",
    "\n",
    "        X_gt, X_syns = get_real_and_synthetic(dataset=dataset,\n",
    "                                            p_train=p_train,\n",
    "                                            n_models=n_models,\n",
    "                                            model_name=model_name,\n",
    "                                            load_syn=load_syn,\n",
    "                                            verbose=verbose,\n",
    "                                            max_n=max_n,\n",
    "                                            nsyn=nsyn)\n",
    "\n",
    "\n",
    "        print(f'Dataset {dataset}\\n')\n",
    "\n",
    "        means, stds, _ = predictive_experiment(X_gt,\n",
    "                                                X_syns,\n",
    "                                                workspace_folder=workspace_folder,\n",
    "                                                results_folder=results_folder,\n",
    "                                                task_type = model_type,\n",
    "                                                save=save,\n",
    "                                                load=load,\n",
    "                                                plot=True,\n",
    "                                                )\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7aab09e613a68d507601146f34912616c121f43e65186c7109754633afa753d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
