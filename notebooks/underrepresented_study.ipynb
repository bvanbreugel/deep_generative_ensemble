{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "from synthcity.metrics.eval_performance import (\n",
    "    PerformanceEvaluatorMLP,\n",
    "    PerformanceEvaluatorXGB,\n",
    ")\n",
    "from synthcity.utils import reproducibility\n",
    "from synthcity.plugins import Plugins\n",
    "import synthcity.logger as log\n",
    "from synthcity.plugins.core.dataloader import GenericDataLoader\n",
    "\n",
    "\n",
    "reproducibility.clear_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Plugins(categories=[\"generic\"]).list()\n",
    "\n",
    "assert device.type == 'cuda'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_generative_ensemble.DGE_data import get_real_and_synthetic\n",
    "from deep_generative_ensemble.DGE_utils import get_folder_names\n",
    "\n",
    "\n",
    "datasets = ['adult', 'moons', 'circles','seer', 'covid', 'breast_cancer'] \n",
    "model_name = 'ctgan_deep'  # synthetic data model\n",
    "    \n",
    "p_train = 0.8  # proportion of training data for generative model. Default values if None\n",
    "n_models = 10  # number of models in ensemble\n",
    "\n",
    "load = True  # results\n",
    "load_syn = True # data\n",
    "save = True  # save results and data\n",
    "max_n = 5000\n",
    "\n",
    "verbose = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_generative_ensemble.DGE_data import load_real_data\n",
    "\n",
    "dataset = 'seer'\n",
    "X = load_real_data(dataset=dataset, max_n=max_n).dataframe()\n",
    "\n",
    "a = X.hist()\n",
    "print(X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occur = X.groupby(['MEDICAL_UNIT']).size()\n",
    "print((occur.loc[4]+occur.loc[12])/occur.sum()*100)\n",
    "print(occur[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthcity.utils import reproducibility\n",
    "\n",
    "dataset = 'seer'\n",
    "print('Dataset:', dataset)\n",
    "nsyn = 5000\n",
    "max_n = 5000\n",
    "model_name = 'ctgan_deep'\n",
    "X_gt, X_syns = get_real_and_synthetic(dataset=dataset,\n",
    "                                p_train=p_train,\n",
    "                                n_models=n_models,\n",
    "                                model_name=model_name,\n",
    "                                load_syn=load_syn,\n",
    "                                verbose=verbose,\n",
    "                                max_n=max_n,\n",
    "                                nsyn=nsyn)\n",
    "\n",
    "\n",
    "nsyn = len(X_syns[0])\n",
    "workspace_folder, results_folder = get_folder_names(dataset=dataset, model_name=model_name, nsyn=nsyn, max_n=max_n)\n",
    "print('Shape of each synthetic dataset:', X_syns[0].shape)\n",
    "print('Target type:', X_gt.targettype)\n",
    "\n",
    "\n",
    "\n",
    "reproducibility.enable_reproducible_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_generative_ensemble.DGE_utils import tt_predict_performance\n",
    "\n",
    "models = []\n",
    "model_real = None\n",
    "model_type = 'xgboost'\n",
    "\n",
    "X_test_r = X_gt.test()\n",
    "X_test_r.targettype = X_gt.targettype\n",
    "X_train_r = X_gt.train()\n",
    "X_train_r.targettype = X_gt.targettype\n",
    "scores_TRTR, model_real = tt_predict_performance(X_test_r, X_train_r, model=model_real, model_type = model_type)\n",
    "\n",
    "scores_TSTS = [0]*len(X_syns)\n",
    "scores_TSTR = [0]*len(X_syns)\n",
    "\n",
    "for i, X_syn in enumerate(X_syns):\n",
    "    X_test, X_train = X_syn.test(), X_syn.train()\n",
    "    X_test.targettype = X_train.targettype = X_gt.targettype\n",
    "    if len(models)==len(X_syns):\n",
    "        model = models[i]\n",
    "    else:\n",
    "        model = None\n",
    "    \n",
    "    scores_TSTS[i], model = tt_predict_performance(X_test, X_train, model=model, model_type = model_type)\n",
    "    scores_TSTR[i], _ = tt_predict_performance(X_test_r, X_train, model=model)\n",
    "            \n",
    "    models.append(model)        \n",
    "\n",
    "scores_TSTR = pd.concat(scores_TSTR, axis=0)\n",
    "scores_TSTS = pd.concat(scores_TSTS, axis=0)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for quantile in [0.5]: #X['MEDICAL_UNIT'].unique()]:\n",
    "    \n",
    "    scores_TSTR_minority = [0]*len(X_syns)\n",
    "    scores_TSTS_minority = [0]*len(X_syns)\n",
    "    if dataset in ['moons', 'circles']:\n",
    "        center = X_test_r.unpack(as_numpy=True)[0].mean(axis=0)\n",
    "        print(center)\n",
    "        dis_to_center = lambda x: np.sum((x.unpack(as_numpy=True)[0]-center)**2,axis=1)\n",
    "        threshold = np.quantile(dis_to_center(X_test_r), quantile)\n",
    "        minority = lambda x: x[dis_to_center(x)>threshold]\n",
    "    elif dataset in ['covid']:\n",
    "        #threshold = X_test_r.data.AGE.quantile(quantile)\n",
    "        #minority = lambda x: x[x.data.AGE>threshold]\n",
    "        minority = lambda x: x[(x['MEDICAL_UNIT']==quantile)]\n",
    "        #minority = lambda x: x[(x['MEDICAL_UNIT']!=12) & (x['MEDICAL_UNIT']!=4)]\n",
    "        #minority = lambda x: x[x.data.DIABETES==1]\n",
    "    elif dataset in ['seer']:\n",
    "        threshold = X_train_r.data.age.quantile(quantile)\n",
    "        minority = lambda x: x[x.data.age>threshold]\n",
    "    elif dataset=='adult':\n",
    "        threshold = X_test_r.data['native.country'].mean()\n",
    "        minority = lambda x: x[x.data['native.country']< threshold]\n",
    "    elif dataset=='breast_cancer':\n",
    "        threshold = X_test_r.data['mean radius'].quantile(1-quantile)\n",
    "        minority = lambda x: x[x.data['mean radius']<threshold]\n",
    "\n",
    "\n",
    "    X_test_r_minority = minority(X_test_r)\n",
    "    X_test_r_minority = GenericDataLoader(X_test_r_minority, target_column='target')\n",
    "    X_test_r_minority.targettype = X_gt.targettype\n",
    "\n",
    "    try:\n",
    "\n",
    "        scores_TRTR_minority, _ = tt_predict_performance(X_test_r_minority, X_train_r, model=model_real, model_type = model_type)\n",
    "        for i, X_syn in enumerate(X_syns):\n",
    "            X_test, X_train = X_syn.test(), X_syn.train()\n",
    "            X_test.targettype = X_train.targettype = X_gt.targettype\n",
    "            X_test_s_minority = minority(X_test)\n",
    "            X_test_s_minority = GenericDataLoader(X_test_s_minority, target_column='target')\n",
    "            X_test_s_minority.targettype = X_gt.targettype\n",
    "            model = models[i]\n",
    "            scores_TSTS_minority[i], _ = tt_predict_performance(X_test_s_minority, X_train, model=model, model_type = model_type)\n",
    "            scores_TSTR_minority[i], _ = tt_predict_performance(X_test_r_minority, X_train, model=model, model_type = model_type)\n",
    "            \n",
    "\n",
    "        scores_TSTR_minority = pd.concat(scores_TSTR_minority, axis=0)\n",
    "        scores_TSTS_minority = pd.concat(scores_TSTS_minority, axis=0)\n",
    "\n",
    "        scores_TRTR = pd.DataFrame(scores_TRTR)\n",
    "        scores_TRTR['Approach'] = 'TRTR'\n",
    "        scores_TSTR = pd.DataFrame(scores_TSTR)\n",
    "        scores_TSTR['Approach'] = 'TSTR'\n",
    "        scores_TSTS = pd.DataFrame(scores_TSTS)\n",
    "        scores_TSTS['Approach'] = 'TSTS'\n",
    "        scores_TRTR_minority = pd.DataFrame(scores_TRTR_minority)\n",
    "        scores_TRTR_minority['Approach'] = 'TRTR'\n",
    "        scores_TSTR_minority = pd.DataFrame(scores_TSTR_minority)\n",
    "        scores_TSTR_minority['Approach'] = 'TSTR'\n",
    "        scores_TSTS_minority = pd.DataFrame(scores_TSTS_minority)\n",
    "        scores_TSTS_minority['Approach'] = 'TSTS'\n",
    "        df = pd.concat([scores_TRTR, scores_TSTR, scores_TSTS],axis=0)\n",
    "        df['Eval'] = 'G'\n",
    "        df_min = pd.concat([scores_TRTR_minority, scores_TSTR_minority, scores_TSTS_minority],axis=0)\n",
    "        df_min['Eval'] = 'M'\n",
    "        df = pd.concat([df, df_min],axis=0)\n",
    "\n",
    "        for metric in ['Acc', 'AUC', 'Precision', 'Recall']:    \n",
    "            import matplotlib.pyplot as plt\n",
    "            import seaborn as sns\n",
    "\n",
    "            plt.figure(figsize=(4,4), tight_layout=True)\n",
    "            sns.catplot(df, x='Eval', y=metric, hue='Approach', kind='box')\n",
    "            name = f'Dataset: {dataset} metric: {metric}, quantile: {quantile}'\n",
    "            filename_plot = f'{dataset}_{quantile}_{metric}.png'\n",
    "            plt.title(name)\n",
    "            #plt.savefig(f'temp/{filename_plot}')\n",
    "            plt.show()\n",
    "    except KeyError:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric= 'Precision'\n",
    "df = pd.concat([scores_TRTR[metric], scores_TSTR[metric], scores_TSTS[metric], scores_TRTR_minority[metric], scores_TSTR_minority[metric], scores_TSTS_minority[metric]],axis=1)\n",
    "df.columns = ['TRTR','TSTR', 'TSTS', \n",
    "                'TRTR (M)','TSTR (M)','TSTS (M)']\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(df)\n",
    "sns.title('Dataset: '+dataset)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric= 'AUC'\n",
    "plotting = sns.boxplot\n",
    "#plotting = lambda x: seaborn.violinplot(x, cut=0)\n",
    "df2 = df[['TSTS', 'TSTS (M)']]\n",
    "df2.columns = ['Overall', 'Minority']\n",
    "plt.figure(figsize=(3,3), dpi=200, tight_layout=True)\n",
    "plotting(df2)\n",
    "plt.ylabel('AUC (synthetic)')\n",
    "plt.savefig(results_folder+'_AUC_synthetic.png')\n",
    "plt.show()\n",
    "plt.figure(figsize=(3,3), dpi=200, tight_layout=True)\n",
    "X = df[['TSTR','TSTR (M)']].to_numpy()-df[['TSTS', 'TSTS (M)']].to_numpy()\n",
    "\n",
    "X = pd.DataFrame(X, columns= ['Overall', 'Minority'])\n",
    "plt.ylabel('AUC (real) - AUC (synthetic)')\n",
    "plotting(X)\n",
    "plt.savefig(results_folder+'_AUC_real_minus_synthetic.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_generative_ensemble.DGE_utils import tt_predict_performance\n",
    "\n",
    "from synthcity.utils import reproducibility\n",
    "\n",
    "dataset = 'seer'\n",
    "print('Dataset:', dataset)\n",
    "nsyn = 2000\n",
    "max_n = 2000\n",
    "dfs = []\n",
    "flag = True\n",
    "\n",
    "for num_layers, model_name in zip([1,2,3],['ctgan_shallow', 'ctgan', 'ctgan_deep']):\n",
    "    X_gt, X_syns = get_real_and_synthetic(dataset=dataset,\n",
    "                                    p_train=p_train,\n",
    "                                    n_models=n_models,\n",
    "                                    model_name=model_name,\n",
    "                                    load_syn=load_syn,\n",
    "                                    verbose=verbose,\n",
    "                                    max_n=max_n,\n",
    "                                    nsyn=nsyn)\n",
    "\n",
    "\n",
    "    nsyn = len(X_syns[0])\n",
    "    workspace_folder, results_folder = get_folder_names(dataset=dataset, model_name=model_name, nsyn=nsyn, max_n=max_n)\n",
    "    print('Shape of each synthetic dataset:', X_syns[0].shape)\n",
    "    print('Target type:', X_gt.targettype)\n",
    "\n",
    "    \n",
    "\n",
    "    reproducibility.enable_reproducible_results()\n",
    "    X_test_r = X_gt.test()\n",
    "    X_test_r.targettype = X_gt.targettype\n",
    "    X_train_r = X_gt.train()\n",
    "    X_train_r.targettype = X_gt.targettype\n",
    "    \n",
    "    if flag:\n",
    "        oracle = tt_predict_performance(X_test_r, X_train_r, model=model_real, model_type = model_type)[0]\n",
    "        flag = False\n",
    "    \n",
    "    models = []\n",
    "    model_real = None\n",
    "    model_type = 'rf'\n",
    "\n",
    "    \n",
    "    scores_TSTS = [0]*len(X_syns)\n",
    "    scores_TSTR = [0]*len(X_syns)\n",
    "    \n",
    "    for i, X_syn in enumerate(X_syns):\n",
    "        X_test, X_train = X_syn.test(), X_syn.train()\n",
    "        X_test.targettype = X_train.targettype = X_gt.targettype\n",
    "        if len(models)==len(X_syns):\n",
    "            model = models[i]\n",
    "        else:\n",
    "            model = None\n",
    "        \n",
    "        scores_TSTS[i], model = tt_predict_performance(X_test, X_train, model=model, model_type = model_type)\n",
    "        scores_TSTR[i], _ = tt_predict_performance(X_test_r, X_train, model=model)\n",
    "                \n",
    "        models.append(model)        \n",
    "\n",
    "    scores_TSTR = pd.concat(scores_TSTR, axis=0)\n",
    "    scores_TSTS = pd.concat(scores_TSTS, axis=0)\n",
    "\n",
    "    scores_TSTR = pd.DataFrame(scores_TSTR)\n",
    "    scores_TSTR['Approach'] = 'TSTR'\n",
    "    scores_TSTS = pd.DataFrame(scores_TSTS)\n",
    "    scores_TSTS['Approach'] = 'TSTS'\n",
    "    df = pd.concat([scores_TSTR, scores_TSTS],axis=0)\n",
    "    df['Hidden layers'] = num_layers\n",
    "    dfs.append(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle[metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'Acc'\n",
    "\n",
    "df = pd.concat(dfs, axis=0)\n",
    "df.rename(columns={'Hidden layers':'CTGAN hidden layers'}, inplace=True)\n",
    "fig = plt.figure(figsize=(3,3), dpi=300, tight_layout=True)\n",
    "sns.catplot(data=df, x='CTGAN hidden layers', y=metric, hue='Approach', kind='box', showfliers = False)\n",
    "# plot a horizontal line at oracle performance\n",
    "#plt.axhline(float(oracle[metric]), color='black', linestyle='--', label='TRTR')\n",
    "os.makedirs('uncertainty_results/stories', exist_ok=True)\n",
    "plt.savefig('uncertainty_results/stories/intro_example.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('test_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f2c99a57bac5efacecfbabca5467a1b952b0d9c1ae060d8550953085427f683"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
