{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "from synthcity.metrics.eval_performance import (\n",
    "    PerformanceEvaluatorMLP,\n",
    "    PerformanceEvaluatorXGB,\n",
    ")\n",
    "from synthcity.utils import reproducibility\n",
    "from synthcity.plugins import Plugins\n",
    "import synthcity.logger as log\n",
    "from synthcity.plugins.core.dataloader import GenericDataLoader\n",
    "\n",
    "\n",
    "reproducibility.clear_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Plugins(categories=[\"generic\"]).list()\n",
    "\n",
    "assert device.type == 'cuda'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'DGE_utils' from '/home/bv292/synthcity/DGE_utils.py'>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import DGE_experiments\n",
    "import DGE_data\n",
    "import DGE_utils\n",
    "importlib.reload(DGE_experiments)\n",
    "importlib.reload(DGE_data)\n",
    "importlib.reload(DGE_utils)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_total 20640 n_train: 4128\n",
      "Shape of each synthetic dataset (4128, 9)\n"
     ]
    }
   ],
   "source": [
    "from DGE_data import get_real_and_synthetic\n",
    "\n",
    "dataset = ['moons', 'cal_housing', 'seer'][1]  # real data\n",
    "model_name = 'ctgan'  # synthetic data model\n",
    "\n",
    "nsyn = None  # number of synthetic data points per synthetic dataset. Defaults to same as generative training size if None\n",
    "p_train = None  # proportion of training data for generative model. Default values if None\n",
    "n_models = 20  # number of models in ensemble\n",
    "load = True  # results\n",
    "\n",
    "load_syn = True  # data\n",
    "save = True  # save results and data\n",
    "verbose = False\n",
    "\n",
    "\n",
    "workspace_folder = os.path.join(\"workspace\", dataset,model_name)\n",
    "results_folder = os.path.join(\"uncertainty_results\",dataset,model_name)\n",
    "\n",
    "X_gt, X_syns = get_real_and_synthetic(dataset=dataset,\n",
    "                                      p_train=p_train,\n",
    "                                      n_models=n_models,\n",
    "                                      model_name=model_name,\n",
    "                                      load_syn=load_syn,\n",
    "                                      verbose=verbose)\n",
    "\n",
    "\n",
    "nsyn = len(X_syns[0])\n",
    "print('Shape of each synthetic dataset:', X_syns[0].shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r2</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DGE (k=20)</th>\n",
       "      <td>0.343863</td>\n",
       "      <td>0.879087</td>\n",
       "      <td>0.682246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DGE (k=10)</th>\n",
       "      <td>0.202780</td>\n",
       "      <td>1.068110</td>\n",
       "      <td>0.694459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DGE (k=5)</th>\n",
       "      <td>0.375684</td>\n",
       "      <td>0.836455</td>\n",
       "      <td>0.678394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive (single)</th>\n",
       "      <td>0.343863</td>\n",
       "      <td>0.879087</td>\n",
       "      <td>0.682246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive (concat)</th>\n",
       "      <td>-0.185865</td>\n",
       "      <td>1.588814</td>\n",
       "      <td>0.688961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oracle</th>\n",
       "      <td>-0.185865</td>\n",
       "      <td>1.588814</td>\n",
       "      <td>0.688961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      r2       mse       mae\n",
       "DGE (k=20)      0.343863  0.879087  0.682246\n",
       "DGE (k=10)      0.202780  1.068110  0.694459\n",
       "DGE (k=5)       0.375684  0.836455  0.678394\n",
       "Naive (single)  0.343863  0.879087  0.682246\n",
       "Naive (concat) -0.185865  1.588814  0.688961\n",
       "Oracle         -0.185865  1.588814  0.688961"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from DGE_experiments import predictive_experiment\n",
    "\n",
    "y_preds, scores = predictive_experiment(X_gt,\n",
    "                      X_syns,\n",
    "                      workspace_folder=workspace_folder,\n",
    "                      results_folder=results_folder,\n",
    "                      save=save,\n",
    "                      load=load)\n",
    "\n",
    "scores\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "We compare the single baseline model vs the generative uncertainty model vs an oracle. Workflow.\n",
    "0. Train and generate synthetic datasets $S_i$.\n",
    "1. Take each synthetic dataset $S_i$ and split it up in train and test.\n",
    "2. Train a model $f_i$ on the train set, for each $S_i$\n",
    "3. Evaluate on the same synthetic dataset's test set $S_{i,test}$, giving $\\hat{M}^S_i$ [Single performance]\n",
    "4. Evaluate on the true real test set (oracle), $D_{test}$, giving $M_i$ [Oracle performance]\n",
    "5. Evaluate on the other synthetic datasets $\\cup_{j\\neq i} S_{j}$, giving $\\hat{M}^G_i$ [Generative performance]\n",
    "6. Compute the deviation from the oracle, $||M_i - \\hat{M}_i||$ and average over all models $f_i$. \n",
    "7. Repeat 1-6 for different model classes $f$\n",
    "\n",
    "N.B. the idea of the above, is that the trained model $f_i$ is the same for each evaluation type. In the model selection section, we will compare the performance of different model classes, where we will train a new model for each evaluation type (hence the aim is to evaluate which class is best, while the model itself may vary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate approaches:\n",
      "                     r2              mse           mae\n",
      "oracle  -85.09 ± 139.01  247.98 ± 186.25  73.83 ± 8.31\n",
      "naive      21.42 ± 16.9    95.61 ± 20.56  73.42 ± 9.05\n",
      "dge       22.15 ± 18.12     95.0 ± 22.11  74.34 ± 9.16\n"
     ]
    }
   ],
   "source": [
    "from DGE_experiments import model_evaluation_experiment\n",
    "\n",
    "if X_gt.targettype is not None:\n",
    "    print('Aggregate approaches:')\n",
    "    res = model_evaluation_experiment(X_gt, X_syns, workspace_folder=workspace_folder, model_type='mlp')[0]\n",
    "    print(res)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "Essentially repeat the above for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  rf      xgboost           svm           knn            lr  \\\n",
      "oracle  66.29 ± 0.68  81.08 ± 0.0  145.73 ± 0.0  148.43 ± 0.0  200.72 ± 0.0   \n",
      "naive   14.94 ± 0.93  10.45 ± 0.0   17.51 ± 0.0   22.32 ± 0.0  118.22 ± 0.0   \n",
      "dge     41.05 ± 0.72  56.68 ± 0.0   19.43 ± 0.0   53.38 ± 0.0   118.5 ± 0.0   \n",
      "\n",
      "               deep_mlp              mlp  \n",
      "oracle  208.78 ± 331.57  247.98 ± 186.25  \n",
      "naive    50.33 ± 102.49  152.91 ± 184.23  \n",
      "dge      55.46 ± 116.19  153.43 ± 184.49  \n",
      "                rf  xgboost     svm     knn      lr  deep_mlp     mlp\n",
      "oracle       66.29    81.08  145.73  148.43  200.72    208.78  247.98\n",
      "naive        14.94    10.45   17.51   22.32  118.22     50.33  152.91\n",
      "DGE          41.05    56.68   19.43   53.38  118.50     55.46  153.43\n",
      "oracle rank   1.00     2.00    3.00    4.00    5.00      6.00    7.00\n",
      "naive rank    2.00     1.00    3.00    4.00    6.00      5.00    7.00\n",
      "DGE rank      2.00     5.00    1.00    3.00    6.00      4.00    7.00\n"
     ]
    }
   ],
   "source": [
    "from DGE_experiments import model_selection_experiment\n",
    "\n",
    "if X_gt.targettype is not None:\n",
    "    if X_gt.targettype == 'classification':\n",
    "        metric = 'accuracy'\n",
    "    elif X_gt.targettype == 'regression':\n",
    "        metric = 'mse'\n",
    "    \n",
    "    results, means_sorted = model_selection_experiment(X_gt, X_syns, relative=False, metric=metric, workspace_folder=workspace_folder, load=load, save=save)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of synthetic data size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's study the effect of synthetic data size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DGE_experiments import predictive_varying_nsyn\n",
    "\n",
    "predictive_varying_nsyn(X_gt, X_syns, dataset, model_name, n_models,\n",
    "                        nsyn, results_folder, workspace_folder, load, save, verbose=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density estimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DGE_experiments import density_experiment\n",
    "\n",
    "if X_gt.targettype is None:\n",
    "    density_experiment(X_gt, X_syns, load, save)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model parameter estimation\n",
    "Using a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation\n",
    "\n",
    "We compare the single baseline model vs the generative uncertainty model. Single workflow\n",
    "1. Take each synthetic dataset $S_i$ and split it up in train and test.\n",
    "2. Train a model $f_i$ on the train set, for each $S_i$\n",
    "3. Evaluate on the same synthetic datasets test set $S_{i,test}$\n",
    "4. Evaluate on the true real test set (oracle), $D_{test}$, giving $\\hat{M}_i$\n",
    "5. Average results across the different synthetic datasets, giving $M_i$.\n",
    "6. Compute the deviation from the oracle, $||M_i - \\hat{M}_i||$ and average.\n",
    "\n",
    "Versus our baseline\n",
    "1. Take each synthetic dataset $S_i$ and split it up in train and test\n",
    "2. Train a model $f_i$ on the train set, for each $S_i$\n",
    "3. Evaluate on the same synthetic datasets test set $S_{i,test}\n",
    "4. Evaluate on the true real test set (oracle), $D_{test}$, giving $\\hat{M}_i$\n",
    "5. Average results across the different synthetic datasets, giving $M_i$.\n",
    "6. Compute the deviation from the oracle, $||M_i - \\hat{M}_i||$ and average.\n",
    "\n",
    "\n",
    "Cross-validation approach to test which type of model would perform best on real data. We compare the single baseline model vs the generative uncertainty model vs an oracle. Workflow Cross-validation.\n",
    "0. Train and generate synthetic datasets $S_i$.\n",
    "1. Use CV to train and evaluate models $f_i$ on each $S_i$. Repeat for all $S_i$. [Single performance]\n",
    "2. Use CV \\textit{over datasets $S_i$} (i.e. train on all but one $S_i$, evaluate on remaining and repeat) to train and evaluate models $f_i$.\n",
    "3. For both cases, evaluate the model also on the true real test set (oracle), $D_{test}$, giving $M_i$ [Oracle performance]\n",
    "4. Compute the deviation from the oracle, $||M_i - \\hat{M}_i||$ and average over all models $f_i$. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('test_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f2c99a57bac5efacecfbabca5467a1b952b0d9c1ae060d8550953085427f683"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
